---
title: "Technical Applications & Data Analytics"
output:
  html_document:
    code_folding: show
    theme:
      bg: '#202123'
      fg: '#B8BCC2'
      primary: '#EA80FC'
      secondary: '#00DAC6'
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
    toc: true
    toc_float: true
    collapsed: true
    smooth_scroll: true
    number_sections: true
    highlight: tango
    df_print: paged
    includes:
      #in_header: header.html
      before_body: prefix.html
      after_body: suffix.html
  printcode:
    printcode: null
    label: 'Display Source-Code:'
    value: yes
---

```{r setup, include = FALSE}
if (requireNamespace("thematic")) thematic::thematic_rmd(font = "auto")
if(!require("knitr")){install.packages("knitr")}
library(knitr)
knitr::opts_chunk$set(
  comment = NA,
  error = FALSE,
  warning = FALSE,
  message = FALSE
)
if(!require("pacman")){ install.packages("pacman")}
pacman::p_load(ourdata, reticulate, VennDiagram, sqldf)
#pacman::p_load(ggplot2, png, VennDiagram, arules, arulesViz, datasets, tcltk, sqldf, gsubfn, proto, RSQLite, ourdata)
```

# R-Package `ourdata`

Das **R-Paket** `ourdata` ist im Rahmen der Vorlesungen in den Studiengängen **Audiovisuelle Medien & Online Publishing** und **Game Design & Management** entstanden. Es enthält einen Großteil der zu Übungszwecken verwendeten Datensätze, sowie einige selbst-programmierte Funktionen mit den sich diese Daten manipulieren bzw. Diagramme ausgeben lassen.

## Daten

Das R-Paket `ourdata` enthält folgende **Daten**:

- `fragebogen` Kopfumfang und andere Merkmale aus dem **GD**- und **AVM-Kurs**.
- `hdi` Weltweiter Index der **menschlichen Entwicklung**.
- `imr` **Säuglingssterblichkeitsraten** weltweit.
- `kirche` **Kirchenaustritte** in Deutschland von *2017* bis *2020*.
- `koelsch` Konsum von **Kölsch** von *2017* bis *2020*.

Mit der **Funktion** `help()` kannst Du Dir **Hilfeseiten** zu allen *Datensätzen* anzeigen lassen. Z.B. `help(fragebogen)` erklärt den Inhalt des Datenpaketes `fragebogen`.

## Funktionen

Das R-Paket `ourdata` enthält folgende **Funktionen**:

- `combine(x, y, ...)` Kombiniert zwei Datensätze mit **ID** und **Fremdschlüsselabgleich**.
- `ourdata()` Druckt eine **Willkommensnachricht** aus.
- `plotter(...)` Zeichnet interaktiv mit variablen Daten verschiedene **Diagramme**.</li>
- `transformer(x, ...)` Transformiert **Werte** vom **Typ** *char* in *numerische Werte*,<br />z.B. `female` zu `1`, `male` zu `2` und `divers` zu `3`.
- `translate(x, target_lang)` Übersetzt Text in die *Zielsprache* mit der **DeepL API**.

Mit der **Funktion** `help()` kannst Du Dir **Hilfeseiten** zu allen *Funktionen* anzeigen lassen. Z.B. `help(plotter)` erklärt die Funktonsweise von der Funktion `plotter()`.

# GM_bac & AVM_bac

In den Kursen **GM-bac5** und **AVM_bac5** im Fach **Technical Applications & Data Analytics** haben wir viele Themen der **Datenanalyse** betrachtetet und uns einige **technische Applikationen** genauer angeschaut. Der Schwerpunkt lag auf **R**, einer Statistik-Programmiersprache, mit der auch dieses Dokument erstellt wurde.

Auf **Ilias** sind die Vorlesungsfolien sowie alle *R-Skripte* und sonstiges Material. Hier findest Du in ansprechender Weise aufbereitet **Diagramme** oder **statistische Methoden** und deren R-Code (dieser liegt zusätzlich auf Ilias im Ordner "*Beispiel-Datensätze, Python und R-Skripte*").

**Viel Spaß hiermit!**

*Benjamin*

## Diagramme {.tabset .tabset-pills}

In `R` oder `RStudio` kann man verschiedene **Diagramm-Typen** zur Visualisation verwenden. Zur Hilfe dient die **Funktion** `plottter()` mit der man alle Diagramme (bis auf das Tortendiagramm) mit eigenen Daten ausführen kann.

```{r AbbIa, echo = FALSE, fig.cap = "Abb. 1.1.a: Heatmap", out.width = '100%'}
include_graphics("images/heatmap.png")
```

Hier eine Auswahl der **Diagramm-Typen**, mit dem dazugehörigen **Code**:
 
### Balkendiagramm

Das **Balkendiagramm** veranschaulicht die Verbindung zwischen einer *numerischen* und einer *kategorialen* **Variablen**. Das Balkendiagramm stellt jede **Kategorie** als **Balken** dar und spiegelt den entsprechenden *numerischen* Wert mit der **Größe** des Balkens wider.

```{r Bar-Plot, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Balkendiagramm erstellen
barplot(kirche$Austritte, kirche$Jahr, main = "Kichenaustritte", col.main = "white", col.lab = "white", yaxt = "n", ylab = "Austritte (per 1.000)", xlab = "Jahreszahlen", names = c("2017", "2018", "2019", "2020"))
# Beschriftung für x- und y-Achse verbessern und die Farbgebung für das Dark-Theme anpassen
axis(1, at = 1:4, lwd = 3, lwd.ticks = 3, col = "white", col.ticks = "white", col.lab = "white", col.axis = "white")
ypos <- seq(0, 600000, by = 100000)
axis(2, at = ypos, labels = sprintf("%1.0f", ypos), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white",  col.axis = "grey")
```

### Box-Diagramm

Das **Box-Diagramm** zeigt die **Verteilung** einer *numerischen* **Variablen** basierend auf fünf zusammenfassenden Statistiken:
- minimaler Nicht-Ausreißer
- erstes Quartil
- Median
- drittes Quartil
- Nicht-Ausreißer

Außerdem zeigen Boxplots die Positionierung von Ausreißern und ob die Daten verzerrt sind.

```{r Box-Plot, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Box-Diagramm erstellen
boxplot(koelsch$Koelsch, main = "Kölschkonsum", col.main = "white", col.lab = "white", yaxt = "n", ylab = "Kölschkonsum in Mil. Litern", xlab = "über den Zeitraum 2017 bis 2020", names = "2020")
# Beschriftung für die y-Achse verbessern und die Farbgebung für das Dark-Theme anpassen
ypos <- seq(160000000, 200000000, by = 10000000)
axis(2, at = ypos, labels = sprintf("%1.0fM.", ypos/1000000), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white",  col.axis = "grey")
```

### Dichtediagramm

Das **Dichtediagramm** zeigt die Verteilung einer *numerischen* **Variablen** über ein **kontinuierliches Intervall**. *Peaks* eines Dichtediagramms visualisieren, wo sich die Werte *numerischer* Variablen **konzentrieren**.

```{r Denstiy-Plot, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Dichtediagramm erstellen
plot(density(fragebogen$alter), main = "Verteilung des Alters im Kurs", col.main = "white", col.lab = "white", yaxt = "n", ylab = "Personen (Dichte)", xlab = "Alter (in Jahren)")
# Beschriftung für die y-Achse verbessern und die Farbgebung für das Dark-Theme anpassen
ypos <- c(0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06)
axis(2, at = ypos, labels = sprintf("%1.0fP", ypos*50), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white",  col.axis = "grey")
```

### Heatmap

Eine **Heatmap*-Diagramm** visualisiert einzelne Werte einer **Matrix** mit **Farben**. **Häufigere Werte** werden typischerweise durch *hellere rötliche* Farben angezeigt und **weniger häufige** Werte werden typischerweise durch *dunklere* Farben angezeigt.

```{r Heatmap, fig.height=4, fig.showtext = TRUE}
data <- matrix(rnorm(81, 0, 9), nrow = 9, ncol = 9)     # Beispiel Daten erstellen
colnames(data) <- paste0("Spalte ", 1:9)                # Spaltennamen setzen
rownames(data) <- paste0("Zeile ", 1:9)                 # Zeilennamen setzen
# Heatmap erstellen
heatmap(data, main = "Heatmap", col.main = "white", col.lab = "white")
```

### Histogramm

Das **Histogramm** gruppiert *kontinuierliche* **Daten** in Bereiche und stellt diese Daten als **Balken** dar. Die **Höhe** jedes Balkens zeigt die **Anzahl** der **Beobachtungen** in jedem Bereich an.

```{r Histogram, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Histogramm erstellen
hist(fragebogen$kopf, main = "Kopfumfänge", col.main = "white", col.lab = "white", ylab = "Personen (Anzahl)", xlab = "Kopfumfang (in cm)")
```

### Liniendiagramm

Das **Liniendiagramm** visualisiert Werte entlang einer **Sequenz** (z.B. *über die Zeit*). Liniendiagramme bestehen aus einer *x-Achse* und einer *y-Achse*. Die x-Achse zeigt normalerweise die **Sequenz** und die y-Achse die Werte an, die jedem Punkt der Sequenz **entsprechen**.

```{r Line-Plot, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Liniendiagramm erstellen
plot(fragebogen$note_mathe, type = "l", main = "Mathenoten", ylab = "Noten", xlab = "Person x",  yaxt = "n", col.main = "white", col.lab = "white")
# Beschriftung für die y-Achse verbessern und die Farbgebung für das Dark-Theme anpassen
ypos <- c(2, 3, 4, 5)
axis(2, at = ypos, labels = sprintf("%1.0f", ypos), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white", col.lab = "grey", col.axis = "white")
```

### Paar-Diagramm

Das **Paar-Diagramm** ist eine Diagrammmatrix, die aus **Streudiagrammen** für jede **Variablenkombination** eines **Datenrahmens** besteht.

```{r Pair-Diagram, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Paar-Diagramm erstellen
pairs(data.frame(fragebogen$interesse, fragebogen$note_annahme), main = "Zusammenhang Interesse und erwarteter Note", labels = c("Interesse", "erwartete Note"), col.main = "white", col.lab = "white")
```

### Qqplot

Ein **QQplot** (oder **Quantil-Quantil-Diagramm**), bestimmt ob *zwei* **Datenquellen** aus einer **gemeinsamen Verteilung** stammen. QQplots ziehen die **Quantile** der beiden *numerischen* Datenquellen gegeneinander. Wenn beide Datenquellen aus derselben Verteilung stammen, fallen die **Punkte** auf einen **Winkel** von *45°*.


```{r Qqplot, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Qqplot erstellen
qqplot(fragebogen$geschlecht, fragebogen$note_mathe, main = "Geschlecht und Mathenote", yaxt = "n", ylab = "Mathenote", xaxt = "n", xlab = "Geschlecht (1 'weiblich', 2 'männlich')", col.main = "white", col.lab = "white")
# Beschriftung für x- und y-Achse verbessern und die Farbgebung für das Dark-Theme anpassen
xpos <- c(1, 2)
axis(1, at = xpos, labels = sprintf("%1.0f", xpos), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white", col.lab = "grey", col.axis = "white")
ypos <- c(2, 3, 4 , 5)
axis(2, at = ypos, labels = sprintf("%1.0f", ypos), lwd = 0.5, lwd.ticks = 1, col = "white", col.ticks = "white", col.lab = "grey", col.axis = "white")
```

### Streudiagramm

Das **Streudiagramm** zeigt zwei *numerische* **Variablen** mit **Punkten** an. Jeder Punkt zeigt den Wert einer Variablen auf der *x-Achse* und den Wert der anderen Variablen auf der *y-Achse* an.

```{r Scatter-Plot, fig.height=4, fig.showtext = TRUE}
# Daten zusammenfügen
df <- combine(imr$name, hdi$country, imr$value, hdi$hdi, col1 = "Land", col2 = "IMR", col3 = "HDI")
# Streudiagramm erstellen
plot(df$HDI, df$IMR, main = "Einfluss des HDI auf IMR", ylab = "IMR", xlab = "HDI", col.main = "white", col.lab = "white")
```

### Tortendiagramm

**Kreis**- oder **Torten-Diagramm**, sind zwar weit verbreitet, weisen jedoch einige Nachteile auf: 

- Unterschiede zwischen den Anteilswerte sind weniger gut erkennbar, da dazu die Fläche der Kreissegmente verglichen werden  muss
- bei vielen Kategorien wird die Darstellung schnell unübersichtlich
- sehr kleine Anteilswerte können oftmals nicht im Kreisdiagramm dargestellt werden

Aufgrund dieser **Nachteile** bietet sich die Verwendung von Kreisdiagramme nur in *selten Fällen* an, meist liefern **Punkt**- oder **Balkendiagramme** bessere Darstellungen.

```{r Pie-Diagram, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Beschriftung Gesamtzahl Jahr mit Prozentangabe erstellen
pie_labels <- paste0(kirche$Austritte, " (", round(100 * kirche$Austritte/sum(kirche$Austritte), 2), "%)")
# Kuchendiagramm erstellen
pie(kirche$Austritte, main = paste0("Kirchenaustritte pro Jahr (insg. ", sum(kirche$Austritte), ")"), labels = pie_labels, col = c("white", "lightblue", "mistyrose", "brown"))
# Legende erstellen
legend("topleft", legend = c("2017", "2018", "2019", "2020"), fill =  c("white", "lightblue", "mistyrose", "brown"))
```

### Venn-Diagramm

Ein **Venn-Diagramm** (oder **Primärdiagramm**; **Mengendiagramm**; **Logikdiagramm**), veranschaulicht alle möglichen *logischen* **Beziehungen** zwischen bestimmten **Datenmerkmalen**. Jedes Merkmal wird als **Kreis** dargestellt, wobei *überlappende* Teile der Kreise **Elemente** darstellen, die beide Merkmale **gleichzeitig aufweisen**.

```{r Venn-Diagram, fig.height=4, fig.showtext = TRUE, results = 'hide', echo = TRUE}
# Triple Venn Diagramm erstellen
draw.triple.venn(area1 = koelsch$Koelsch[4], area2 = kirche$Austritte[4], area3 = 1000000, n12 = 220000, n23 = 50000, n13 = 600000, n123 = 40000, main = "Kölsch -> Kirchenaustritt -> Köln?", fill = c("yellow", "brown", "blue"), category = c("Kölsch", "Kirche", "Köln"), main.col = "white", sub.col = "white", col = "white")
```

## Data Mining {.tabset .tabset-pills}

Die **Methoden** des **Data Mining** lassen sich grundsätzlich in die Gruppen *Klassifikation*, *Prognose*, *Segmentierung* und *Abhängigkeitsentdeckung* einteilen. Dazu kommen **Algorithmen** zum Einsatz. 

*Ein Algorithmus ist eine formale Handlungsvorschrift zur Lösung von Instanzen einer bestimmten Problemklasse.* ^[Sauer [2019]]

```{r AbbIIa, echo = FALSE, fig.cap = "Abb. 2.2.a: Heatmap", out.width = '50%'}
include_graphics("images/Gruppen_DataMining.png")
```

Hier findest Du die Data **Mining Methoden** aus den Vorlesungen, mit dem dazugehörigen **Code**:

### Korrelation Analyse

Will man einen **Zusammenhang** zwischen zwei **metrischen Variablen** untersuchen, zum Beispiel zwischen dem Alter und dem Gewicht von Kindern, so berechnet man eine **Korrelation**. Diese besteht aus einem **Korrelationskoeffizienten** (`rho` bei Spearman) und einem **p-Wert**.

Der **Korrelationskoeffizient** gibt die Stärke und die Richtung des Zusammenhangs an. Er liegt zwischen `-1` und `1`. Ein Wert nahe `-1` bezeichnet einen starken **negativen Zusammenhang** (z.B. *„Mehr zurückgelegte Strecke mit dem Auto, weniger Treibstoff im Tank“*). Ein Wert nahe `1` spricht für einen starken **positiven Zusammenhang** (z.B. *„mehr Futter, dickere Kühe“*). Kein Zusammenhang besteht, wenn der Wert nahe `0` liegt. 

Der **p-Wert** sagt aus, ob es einen **signifikanten Zusammenhang** gibt. **p-Werte** kleiner als `0,05` werden als **statistisch signifikant** bezeichnet.

```{r Korrelationsanalyse, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Beide Listen mit 'SQL JOIN' kombinieren
imr_hdi <- sqldf('SELECT imr.name AS "country", imr.value As "imr", hdi.hdi AS "hdi" FROM imr INNER JOIN hdi ON imr.name = hdi.country ORDER BY imr.value DESC')

# Streudiagramm (Scatterplot)
plot(imr_hdi$imr ~ imr_hdi$hdi, main = "HDI IMR Korrelation", ylab = "Säuglingssterblichkeit (per 1.000)", xlab = "Human Development Index", xlim = range(0:1), ylim = range(1:110))

# Quantifizierung der Zusammenhänge mittels Spearman Korrelation Funktion
cor.test(imr_hdi$imr, imr_hdi$hdi, method="spearman", exact=FALSE)
```

### Linear Regress Analyse

**Lineare Regression** ist eines der nützlichsten Werkzeuge in der Statistik. Regressionsanalyse erlaubt es **Zusammenhänge** zwischen Parametern zu schätzen und somit ein erklärendes Model für das Auftreten gewisser Phänomene zu geben. Wirkliche Kausalität wird durch statistische Analysen dieser Art zwar nicht aufgedeckt, die Ergebnisse aus solchen Analysen können aber Hinweise in diese Richtung geben. ^[Wozabal [2007]]

```{r LinearRegressAnalyse, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Mittels Funktion 'combine' aus dem R-Paket 'ourdata' einen vergleichbaren Datensatz erstellen mit IMR und HDI
df <- combine(imr$name, hdi$country, imr$value, hdi$hdi, col1 = "Land", col2 = "IMR", col3 = "HDI")

# Lineares Modell mit 'lm' erstellen
mdl <- lm(IMR ~ HDI, data=df)

# 'summary' Funktion
summary(mdl)

# Den p-Wert für 'HDI' errechnen
matCoef <- summary(mdl)$coefficients
pval <- matCoef["HDI", 4]
print(paste0("Der Effekt vom HDI auf IMR ist statistisch signifikant p = ", round(pval, 2), " (", pval, ")."))

# Diagramm erstellen
plot(df$HDI, df$IMR, xlab = "Prädiktor", ylab = "Ergebnis", col = "darkblue", pch = 16, main = "Lineare Regression")
# Regressionsgrade erstellen
abline(mdl, col = "darkred")
```

### Basket Analyse

Die **Basket Analyse** kann zur Entdeckung von **Assoziationen** und **Korrelationen** zwischen Elementen in riesigen transaktionalen oder relationalen Datensätzen führen.

Das Aufsuchen von **Verbindungen** zwischen verschiedenen Artikeln, die Kunden in ihre „Warenkörbe“ legen ist ein häufiger Einsatz der Analyse. Das Wissen um diese Assoziationen kann für Einzelhändler oder Vermarkter hilfreich sein, um Marketingstrategien zu entwickeln. Das geschieht, indem sie Erkenntnisse darüber gewinnen, welche Artikel von Kunden häufig zusammen gekauft werden.

Wenn Kunden beispielsweise Milch kaufen, *wie wahrscheinlich ist es, dass sie auf derselben Fahrt zum Supermarkt auch Brot (und welche Brotsorte) kaufen?* Diese Informationen können zu einer Umsatzsteigerung führen, indem sie Einzelhändlern helfen, **selektives Marketing** zu betreiben und ihren Verkaufsraum zu planen.

```{r BasketAnalyse, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Data frame laden
#data(Groceries)

# Frequenz Diagramm für die Top 20 Artikel erstellen
#itemFrequencyPlot(Groceries, topN = 20, type = "absolute", horiz = TRUE)
# nach Milch suchen
#itemFrequency(Groceries)[grep("milk", itemLabels(Groceries))]

# Die Regeln ableiten
#rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8))

# Zeige die Top 5 Regeln (nur 2 Nachkommastellen)
#options(digits=2)
#inspect(rules[1:5])
#rules <- sort(rules, by="confidence", decreasing = TRUE)
#rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8, maxlen=3))

#subset.matrix <- is.subset(rules, rules)
#subset.matrix[lower.tri(subset.matrix, diag = T)] <- NA
#redundant <- colSums(subset.matrix, na.rm = T) >= 1
#rules.pruned <- rules[!redundant]
#rules<-rules.pruned

# Diagramme ausgeben
#plot(rules, method = "graph", engine= "interactive", shading = NA)
#plot(rules, method = "graph", engine = "htmlwidget")
```

### Cluster Analyse K-Means

```{r K-Means, fig.height=4, fig.showtext = TRUE, results = 'hold'}

```

### Neuronale Netze-Analyse

```{r NeuronaleNetze-Analyse, fig.height=4, fig.showtext = TRUE, results = 'hold'}

```

## Sonstiges {.tabset .tabset-pills}

Hier findest Du **Code-Beispiele**, **Lustiges** und **Neuigkeiten** rund um das Thema *technische Applikationen & Datenanalyse*:

### Beispiel R-Code

Dieses Beispiel zeigt die Verwendung von **Schleifen** in R. Schleifen kommen in der **Programmierung** häufig zum Einsatz, z.B. zum Überprüfen von **Übereinstimmungen** in zwei verschiedenen Datensätzen.

```{r RloopCode, code = readLines(system.file("extdata", "Komplexeres_R_Skript.R", package = "ourdata", mustWork = FALSE))}
```

### Ext. R-Code ausführen

Dieser **Code** steht in der Datei `example.R` und wir von **R-Markdown** eingelesen und ausgeführt.

```{r ExtCode, echo = FALSE, fig.height=4, fig.showtext = TRUE, results = 'hold'}
read_chunk(system.file("extdata", "example.R", package = "ourdata", mustWork = FALSE))
```
```{r variablesXY}
```
```{r plotXY}
```

### DeepL Übersetzung (P)

Es gibt einen sehr interessanten Artikel auf [https://trends.rbc.ru](https://trends.rbc.ru) zum Thema ***´Big Data**. Nur leider in russischer Sprache :(... 

**Textauszug:**

Что такое Big Data?

Big Data или большие данные — это структурированные или неструктурированные массивы данных большого объема. Их обрабатывают при помощи специальных автоматизированных инструментов, чтобы использовать для статистики, анализа, прогнозов и принятия решений.

Сам термин «большие данные» предложил редактор журнала Nature Клиффорд Линч в спецвыпуске 2008 года [ ^1^ ](https://www.nature.com/nature/volumes/455/issues/7209). Он говорил о взрывном росте объемов информации в мире. К большим данным Линч отнес любые массивы неоднородных данных более 150 Гб в сутки, однако единого критерия до сих пор не существует.


Hier wird gezeigt wie **Python Code** direkt ausgeführt werden kann:

```{python pDeepL, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# Python Bibliothek 'DeepL' laden
import deepl

# Übersetzung mit DeepL API (Python)
translator = deepl.Translator("c52a9c7d-3198-063c-2bbf-8f67173820ce:fx")
result = translator.translate_text("Что такое Big Data? Big Data или большие данные\n — это структурированные или неструктурированные массивы данных большого объема. \nИх обрабатывают при помощи специальных автоматизированных инструментов, \nчтобы использовать для статистики, анализа, \nпрогнозов и принятия решений. \nСам термин «большие данные» предложил редактор журнала Nature \nКлиффорд Линч в спецвыпуске 2008 года. \nОн говорил о взрывном росте объемов информации в мире. \nК большим данным Линч отнес любые массивы неоднородных данных более 150 Гб в сутки, \nоднако единого критерия до сих пор не существует.", target_lang="DE")

# Übersetzung ausgeben
print(result)
```

### DeepL Übersetzung (R)

Dieser Artikel kann [hier](https://trends.rbc.ru/trends/innovation/5d6c020b9a7947a740fea65c) vollständig gelesen werden ;)

**Textauszug (Fortsetzung):**

ЧДо 2011 года анализом больших данных занимались только в рамках научных и статистических исследований. Но к началу 2012-го объемы данных выросли до огромных масштабов, и возникла потребность в их систематизации и практическом применении.

С 2014 на Big Data обратили внимание ведущие мировые вузы, где обучают прикладным инженерным и ИТ-специальностям. Затем к сбору и анализу подключились ИТ-корпорации — такие, как Microsoft, IBM, Oracle, EMC, а затем и Google, Apple, Facebook и Amazon. Сегодня большие данные используют крупные компании во всех отраслях, а также — госорганы. Подробнее об этом — в материале [Кто и зачем собирает большие данные?](https://trends.rbc.ru/trends/industry/5ea9bfdf9a7947678acbf8b1)

Hier wird gezeigt wie eine **Python Funktion** aus **R** ausgeführt werden kann:

```{r rDeepL, fig.height=4, fig.showtext = TRUE, results = 'hold'}
# R lädt mit 'reticulate::source_python' die Python Datei 'py_deepl.py'
source_python(system.file("extdata", "py_deepl.py", package = "ourdata", mustWork = FALSE))

# Ausführen der Python Funktion 'py_deepl'
py_deepl("ЧДо 2011 года анализом больших данных занимались только в рамках научных и статистических исследований. \nНо к началу 2012-го объемы данных выросли до огромных масштабов, \nи возникла потребность в их систематизации и практическом применении. \nС 2014 на Big Data обратили внимание ведущие мировые вузы, \nгде обучают прикладным инженерным и ИТ-специальностям. \nЗатем к сбору и анализу подключились ИТ-корпорации — такие, как Microsoft, \nIBM, Oracle, EMC, а затем и Google, Apple, Facebook и Amazon. \nСегодня большие данные используют крупные компании во всех отраслях, а также — госорганы. \nПодробнее об этом — в материале Кто и зачем собирает большие данные?", "DE")
```

### DeepL R-Code

Dies ist der Übersetzungs-Code der **DeepL API** in **R** geschrieben:

```{r deepLRCode, code = readLines(system.file("extdata", "translate.R", package = "ourdata", mustWork = FALSE))}
```

### DeepL Python-Code

Dies ist der Übersetzungs-Code der **DeepL API** in **Python** geschrieben:

```{python DeepLPythonCode, code = readLines(system.file("extdata", "py_deepl.py", package = "ourdata", mustWork = FALSE))}
```

### Katzen-Statistik

1000 **Wissenschaftler** haben ganz *unabhängig* voneinander (unter der Einhaltung aller wissenschaftlichen Voraussetzungen) herausgefunden:

```{r AbbIIIa, echo = FALSE, fig.cap = "Abb. 2.3.a: Katzen-Statistik", out.width = '100%'}
include_graphics("images/cat.png")
```

### Big Data-Jobs

Ein [spannender Artikel](https://www.thebalancecareers.com/top-7-big-data-jobs-4588947) der die Jobs im **Big Data** Bereich auflistet und mögliche Gehälter (in Dollar) auflistet:

```{r AbbIIIb, echo = FALSE, fig.cap = "Abb. 2.3.b: Big Data Jobs", out.width = '100%'}
include_graphics("images/Top_7_Big_Data_Jobs.pdf")

url <- as.character(system.file("rmd/images", "Top_7_Big_Data_Jobs.pdf", package = "ourdata", mustWork = FALSE))
browseURL(url, browser = getOption("browser"), encodeIfNeeded = FALSE)
```

# Literaturverzeichnis {.tabset .unnumbered}

Hier ist die **Literatur** die im Fach *Technical Applications & Data Analytics* zum Einsatz kam aufgelistet:
<br /><br />
<ul>
<li>Cleve, J/Lämmel, U. [2020]<br />
*Data Mining*, 3. Auflage, Berlin/Boston 2020</li>
<br />
<li>Romeijn, J. W. [2016]<br />
*Philosophy of Statistics.* In E. N. Zalta (Hrsg.), The Stanford Encyclopedia of Philosophy, Stanford 2016</li>
<br />
<li>Sauer, S. [2019]<br />
*Moderne Datenanalyse mit R*, Wiesbaden 2019</li>
<br />
<li>Tukey, J. W. [1962]<br />
*The future of data analysis. The Annals of Mathematical Statistics*, Vol. 33, No. 1, Institute of Mathematical Statistics 1962</li>
<br />
<li>Wickham, H./Garrett G. [2017]<br />
*R for Data Science*, O‘Reilly Media 2017</li>
<br />
<li>Wozabal, D. [2007]<br />
*Statistisches Programmieren – Regressionen in R (Session 6)*, 2007</li>
</ul>
